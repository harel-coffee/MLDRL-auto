{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import  numpy\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as  V\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(before,after):\n",
    "    gtvt1 = pd.read_csv('before.csv')\n",
    "    gtvt2 = pd.read_csv('after.csv')\n",
    "\n",
    "\n",
    "\n",
    "    X1, y1 = gtvt1.loc[:, [str(before) for before in range(1,82)]].values, gtvt1.loc[:, ['Label']].values\n",
    "\n",
    "    X2, y2 = gtvt2.loc[:, [str(after) for after in range(1,82)]].values, gtvt2.loc[:, ['Label']].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(X1)\n",
    "    X2 = scaler.fit_transform(X2)\n",
    "\n",
    "\n",
    "    X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X1, y1, test_size=0.2, shuffle=True)\n",
    "\n",
    "    X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, y2, test_size=0.2, shuffle=True)\n",
    "    \n",
    "    return X_train1, X_test1, Y_train1, Y_test1,X_train1, X_test1, Y_train1, Y_test1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_stream_dataloader(X_train1,Y_train1,X_test1,Y_test1,X_train2,Y_train2,X_test2,Y_test2):\n",
    "\n",
    "\n",
    "\n",
    "    X_1_train = torch.from_numpy(X_train1.astype(numpy.float32))\n",
    "\n",
    "\n",
    "    y_1_train = torch.from_numpy(Y_train1.astype(numpy.int))\n",
    "    X_1_test = torch.from_numpy(X_test1.astype(numpy.float32))\n",
    "    y_1_test = torch.from_numpy(Y_test1.astype(numpy.int))\n",
    "\n",
    "    X_2_train = torch.from_numpy(X_train2.astype(numpy.float32))\n",
    "    y_2_train = torch.from_numpy(Y_train2.astype(numpy.int))\n",
    "    X_2_test = torch.from_numpy(X_test2.astype(numpy.float32))\n",
    "    y_2_test = torch.from_numpy(Y_test2.astype(numpy.int))\n",
    "\n",
    "    my_dataset1 = TensorDataset(X_1_train,X_2_train,y_1_train)\n",
    "    my_dataset2 = TensorDataset(X_1_test, X_2_test, y_1_test)\n",
    "\n",
    "    my_dataset_train = DataLoader(my_dataset1,batch_size=5,shuffle=False,pin_memory=True)\n",
    "    my_dataset_test = DataLoader(my_dataset2, batch_size=5, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return my_dataset_train,my_dataset_test\n",
    "\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "\n",
    "def calAUC(prob,labels):\n",
    "    f = list(zip(prob,labels))\n",
    "    rank = [values2 for values1,values2 in sorted(f,key=lambda x:x[0])]\n",
    "    rankList = [i+1 for i in range(len(rank)) if rank[i]==1]\n",
    "    posNum = 0\n",
    "    negNum = 0\n",
    "    for i in range(len(labels)):\n",
    "        if(labels[i]==1):\n",
    "            posNum+=1\n",
    "        else:\n",
    "            negNum+=1\n",
    "    auc = 0\n",
    "    auc = (sum(rankList)- (posNum*(posNum+1))/2)/(posNum*negNum)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_augment_data(X_train,y_train):\n",
    "    import pandas as pd\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    # x_columns = X_train.columns\n",
    "    # y_columns = y_train.columns\n",
    "    #注意过采样时 只对训练集进行过采样\n",
    "    oversampler = SMOTE()\n",
    "    x_smote_train,y_smote_train= oversampler.fit_sample(X_train,y_train)\n",
    "    # x_smote_train = pd.DataFrame(x_smote_train, columns=x_columns)\n",
    "    # y_smote_train = pd.DataFrame(y_smote_train, columns=y_columns)\n",
    "    return x_smote_train,y_smote_train\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def feature_selection_rf(X_train, Y_train):\n",
    "    model = RandomForestRegressor(random_state=1, max_depth=200)\n",
    "    model.fit(X_train, Y_train.values.ravel())\n",
    "    features = X_train.columns\n",
    "    importances = model.feature_importances_\n",
    "    rf_indices = np.argsort(importances)[-81:]  # top 10 features\n",
    "#     rf_indices = np.argwhere(importances>0.0003)\n",
    "    \n",
    "    for f in range(rf_indices.shape[0]):\n",
    "       print(\"%d. feature %d (%f)\" % (f + 1, rf_indices[f], importances[rf_indices[f]]))\n",
    "    #plt.title('Feature Importances')\n",
    "    #plt.barh(range(len(rf_indices)), importances[rf_indices], color='b', align='center')\n",
    "    #plt.yticks(range(len(rf_indices)), [features[i] for i in rf_indices])\n",
    "    #plt.xlabel('Relative Importance')\n",
    "    #plt.show()\n",
    "    rf_features = [features[i] for i in rf_indices]\n",
    "    return rf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLDRL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as  V\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import  numpy\n",
    "from  sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "\n",
    "\n",
    "class Normal(object):\n",
    "    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma  # either stdev diagonal itself, or stdev diagonal from decomposition\n",
    "        self.logsigma = log_sigma\n",
    "        dim = mu.get_shape()\n",
    "        if v is None:\n",
    "            v = torch.FloatTensor(*dim)\n",
    "        if r is None:\n",
    "            r = torch.FloatTensor(*dim)\n",
    "        self.v = v\n",
    "        self.r = r\n",
    "\n",
    "\n",
    "class Encoder1(torch.nn.Module):\n",
    "    def __init__(self, D_in, H):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return F.relu(self.linear1(x))\n",
    "\n",
    "\n",
    "class Decoder1(torch.nn.Module):\n",
    "    def __init__(self, latent, H, D_out):\n",
    "        super(Decoder1, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(latent, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.relu(self.linear2(x))\n",
    "\n",
    "\n",
    "class Encoder2(torch.nn.Module):\n",
    "    def __init__(self, D_in, H):\n",
    "        super(Encoder2, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return F.relu(self.linear1(x))\n",
    "\n",
    "\n",
    "class Decoder2(torch.nn.Module):\n",
    "    def __init__(self, latent, H, D_out):\n",
    "        super(Decoder2, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(latent, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        return F.relu(self.linear2(x))\n",
    "\n",
    "\n",
    "class MLDRL(torch.nn.Module):\n",
    "    latent_dim = 10\n",
    "\n",
    "    def __init__(self, encoder1, decoder1,encoder2,decoder2):\n",
    "        super(MLDRL, self).__init__()\n",
    "        self.encoder1 = encoder1\n",
    "        self.decoder1 = decoder1\n",
    "        self.encoder2 = encoder2\n",
    "        self.decoder2 = decoder2\n",
    "        self._enc_mu = torch.nn.Linear(32, 10)\n",
    "        self._enc_log_sigma = torch.nn.Linear(32, 10)\n",
    "\n",
    "        self.fc1 = nn.Linear(14, 2, bias=True)\n",
    "\n",
    "        self.act = F.sigmoid\n",
    "\n",
    "    def _sample_latent(self, h_enc):\n",
    "        \"\"\"\n",
    "        Return the latent normal sample z ~ N(mu, sigma^2)\n",
    "        \"\"\"\n",
    "        mu = self._enc_mu(h_enc)\n",
    "        log_sigma = self._enc_log_sigma(h_enc)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
    "\n",
    "        self.z_mean = mu\n",
    "        self.z_sigma = sigma\n",
    "\n",
    "        return mu + sigma * Variable(std_z, requires_grad=False), mu, sigma  # Reparameterization trick\n",
    "\n",
    "    def forward(self,x1,x2):\n",
    "        h_enc1 = self.encoder1(x1)\n",
    "        z1, mu1, sigma1 = self._sample_latent(h_enc1)\n",
    "\n",
    "        comm1, spe1 = z1.split([6,4], dim=1)\n",
    "        decoder1_ = self.decoder1(z1)\n",
    "\n",
    "        h_enc2 = self.encoder2(x2)\n",
    "        z2, mu2, sigma2 = self._sample_latent(h_enc2)\n",
    "        comm2, spe2 = z2.split([6,4], dim=1)\n",
    "\n",
    "        decoder2_ = self.decoder2(z2)\n",
    "\n",
    "        connect1 = torch.cat([spe1,comm2],dim=1)\n",
    "        decoder3_ = self.decoder1(connect1)\n",
    "        connect2 = torch.cat([comm1,spe2],dim=1)\n",
    "        decoder4_ = self.decoder2(connect2)\n",
    "\n",
    "        inputmlp_com = (comm1 + comm2) / 2\n",
    "        inputmlp = torch.cat((inputmlp_com, spe1, spe2), 1)\n",
    "\n",
    "        out = self.act(self.fc1(inputmlp))\n",
    "\n",
    "        out = F.softmax(out)\n",
    "\n",
    "        return decoder1_,decoder2_,decoder3_,decoder4_,comm1,spe1,comm2,spe2,mu1,sigma1,mu2,sigma2,out,inputmlp\n",
    "\n",
    "def latent_loss(z_mean, z_stddev):\n",
    "    mean_sq = z_mean * z_mean\n",
    "    stddev_sq = z_stddev * z_stddev\n",
    "    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test():\n",
    "    \n",
    "    Weightloss1 = torch.tensor(torch.FloatTensor([1.4]), requires_grad=True)\n",
    "    Weightloss2 = torch.tensor(torch.FloatTensor([3]), requires_grad=True)\n",
    "    Weightloss3 = torch.tensor(torch.FloatTensor([0.4]), requires_grad=True)\n",
    "\n",
    "    params = [Weightloss1, Weightloss2, Weightloss3]\n",
    "\n",
    "\n",
    "    auc_list = []\n",
    "    acc_list = []\n",
    "    speci_list = []\n",
    "    sen_list = []\n",
    "\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    Gradloss = nn.L1Loss()\n",
    "    lam = []\n",
    "    beita = []\n",
    "    gama = []\n",
    "    input_dim = 81\n",
    "    l_list = []\n",
    "    MLP = MLP()\n",
    "    alph = 0.2\n",
    "    dataloader,dataloader2 = get_two_stream_dataloader(X_train1,Y_train1,X_test1,Y_test1,X_train2,Y_train2,X_test2,Y_test2)\n",
    "    encoder1 = Encoder1(input_dim,32)\n",
    "    decoder1 = Decoder1(10, 32, input_dim)\n",
    "    encoder2 = Encoder2(input_dim, 32)\n",
    "    decoder2 = Decoder2(10, 32, input_dim)\n",
    "\n",
    "    vae = MLDRL(encoder1,decoder1,encoder2,decoder2)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "    opt1 = optim.Adam(vae.parameters(), lr=0.000085)\n",
    "\n",
    "    opt2 = torch.optim.Adam(params, lr=0.000085)\n",
    "    l = None\n",
    "\n",
    "    loss_comm=None\n",
    "    loss_spe = None\n",
    "    loss_comm_spe = None\n",
    "    loss_class = None\n",
    "    com_list = []\n",
    "    spe_list = []\n",
    "    com_spe_list = []\n",
    "    classfication = []\n",
    "    com_list_ = []\n",
    "    spe_list_ = []\n",
    "    com_spe_list_ = []\n",
    "    classfication_ = []\n",
    "    l_list_ = []\n",
    "    Hiddle = []\n",
    "\n",
    "    for epoch in range(50):\n",
    "        for i, (data1,data2,target)  in enumerate(dataloader):\n",
    "            sum_recon=0\n",
    "            sum_spe =0\n",
    "            sum_com =0\n",
    "            sum_com_spe =0\n",
    "            sum_class = 0\n",
    "            inputs1 = data1\n",
    "            inputs2 = data2\n",
    "\n",
    "\n",
    "            d1,d2,d3,d4,c1,s1,c2,s2,m1,sig1,m2,sig2,pred, hiddle_final = vae(V(inputs1),V(inputs2))\n",
    "\n",
    "            # inputmlp_com = (c1+c2)/2\n",
    "            # inputmlp  = torch.cat((inputmlp_com,s1,s2),1)\n",
    "            #\n",
    "            # pred = MLP(inputmlp)\n",
    "\n",
    "            target = target.squeeze()\n",
    "\n",
    "\n",
    "            loss_cass = criterion1(pred,target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            loss_comm = criterion(c1, c2)\n",
    "\n",
    "            loss_spe = criterion(s1, s2)\n",
    "\n",
    "            loss_comm_spe = loss_comm-loss_spe\n",
    "\n",
    "            sum_com = sum_com+loss_comm\n",
    "            sum_spe = sum_spe+loss_spe\n",
    "            sum_com_spe = sum_com_spe+loss_comm_spe\n",
    "            sum_class = sum_class+loss_cass\n",
    "\n",
    "            ll1 = latent_loss(m1, sig1)\n",
    "            ll2 = latent_loss(m2, sig2)\n",
    "\n",
    "            loss1 = criterion(d1, inputs1)\n",
    "            loss2 = criterion(d2, inputs2)\n",
    "            loss3 = criterion(d3, inputs1)\n",
    "            loss4 = criterion(d4, inputs2)\n",
    "            recon = loss1+loss2+loss3+loss4\n",
    "            KL = ll1+ll2\n",
    "            sum_recon = sum_recon+recon\n",
    "            l1 = params[0]*(recon+KL)\n",
    "            l2 = params[1]*(loss_comm_spe)\n",
    "            l3 = params[2]*(loss_cass)\n",
    "\n",
    "\n",
    "            mean_loss = torch.div(torch.add(torch.add(l1,l2),l3),3)\n",
    "\n",
    "            if epoch ==0:\n",
    "                l01 = l1.data\n",
    "                l02 = l2.data\n",
    "                l03 = l3.data\n",
    "\n",
    "            opt1.zero_grad()\n",
    "\n",
    "            mean_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Getting gradients of the first layers of each tower and calculate their l2-norm\n",
    "\n",
    "            param = list(vae.parameters())\n",
    "            G1R_1 = torch.autograd.grad(l1, param[13], retain_graph=True, create_graph=True)\n",
    "            G1R_2 = torch.autograd.grad(l1, param[15], retain_graph=True, create_graph=True)\n",
    "            G1R = torch.div(torch.add(G1R_1[0],G1R_2[0]),2)\n",
    "            G1 = torch.norm(G1R, 2)\n",
    "\n",
    "            G2R_1 = torch.autograd.grad(l2, param[13], retain_graph=True, create_graph=True)\n",
    "            G2R_2 = torch.autograd.grad(l2, param[15], retain_graph=True, create_graph=True)\n",
    "            G2R = torch.div(torch.add(G2R_1[0], G2R_2[0]), 2)\n",
    "            G2 = torch.norm(G2R, 2)\n",
    "\n",
    "\n",
    "            G3R_1 = torch.autograd.grad(l3, param[13], retain_graph=True, create_graph=True)\n",
    "            G3R_2 = torch.autograd.grad(l3, param[15], retain_graph=True, create_graph=True)\n",
    "            G3R = torch.div(torch.add(G3R_1[0], G3R_2[0]), 2)\n",
    "            G3 = torch.norm(G3R, 2)\n",
    "\n",
    "            G_avg = torch.div(torch.add(torch.add(G1, G2), G3), 3)\n",
    "\n",
    "\n",
    "            # Calculating relative losses\n",
    "\n",
    "            lhat1 = torch.div(l1, l01)\n",
    "            lhat2 = torch.div(l2, l02)\n",
    "            lhat3 = torch.div(l3, l03)\n",
    "\n",
    "            lhat_avg = torch.div(torch.add(torch.add(lhat1, lhat2),lhat3), 3)\n",
    "\n",
    "            # Calculating relative inverse training rates for tasks\n",
    "            # 分别表示不同任务反向传播的速度\n",
    "            inv_rate1 = torch.div(lhat1, lhat_avg)\n",
    "            inv_rate2 = torch.div(lhat2, lhat_avg)\n",
    "            inv_rate3 = torch.div(lhat3, lhat_avg)\n",
    "\n",
    "\n",
    "            # Calculating the constant target for Eq. 2 in the GradNorm paper\n",
    "            C1 = G_avg * (inv_rate1) ** alph\n",
    "            C2 = G_avg * (inv_rate2) ** alph\n",
    "            C3 = G_avg * (inv_rate3) ** alph\n",
    "\n",
    "\n",
    "            C1 = C1.detach()\n",
    "            C2 = C2.detach()\n",
    "            C3 = C3.detach()\n",
    "\n",
    "\n",
    "            opt2.zero_grad()\n",
    "            Lagrad = torch.add(torch.add(Gradloss(G1,C1),Gradloss(G2,C2)),Gradloss(G3,C3))\n",
    "\n",
    "            Lagrad.backward()\n",
    "\n",
    "            opt2.step()\n",
    "            opt1.step()\n",
    "\n",
    "            if epoch==49:\n",
    "                if i ==0:\n",
    "                    c_final = (c1+c2)/2\n",
    "                    sp1 = s1\n",
    "                    sp2 = s2\n",
    "                else:\n",
    "                    c_final = torch.cat([c_final,(c1+c2)/2],dim=0)\n",
    "                    sp1 = torch.cat([sp1,s1],dim=0)\n",
    "                    sp2 = torch.cat([sp2,s2],dim=0)\n",
    "                latent_space = torch.cat([c_final,sp1,sp2],dim=1)\n",
    "\n",
    "\n",
    "        if Weightloss1<0.1:\n",
    "            Weightloss1=0.01\n",
    "\n",
    "        correct = 0\n",
    "        true_test= []\n",
    "        pred_test= []\n",
    "        pred_pro = []\n",
    "\n",
    "        total = len(dataloader2.dataset)\n",
    "        for i, (data3, data4, target3) in enumerate(dataloader2):\n",
    "            sum_recon_ = 0\n",
    "            sum_spe_ = 0\n",
    "            sum_com_ = 0\n",
    "            sum_com_spe_ = 0\n",
    "            sum_class_ = 0\n",
    "\n",
    "            inputs3 = data3\n",
    "            # print(inputs1)\n",
    "            inputs4 = data4\n",
    "            # print(inputs2)\n",
    "\n",
    "            d1_, d2_, d3_, d4_, c1_, s1_, c2_, s2_, m1_, sig1_, m2_, sig2_ ,pred,hiddle_final = vae(V(inputs3), V(inputs4))\n",
    "\n",
    "            # inputmlp_com_ = (c1_ + c2_) / 2\n",
    "            # inputmlp_ = torch.cat((inputmlp_com_, s1_, s2_), 1)\n",
    "            #\n",
    "            # pred = MLP(inputmlp_)\n",
    "\n",
    "            if epoch == 49:\n",
    "                for i in range(len(hiddle_final.detach().numpy())):\n",
    "                    Hiddle.append(hiddle_final.detach().numpy()[i])\n",
    "\n",
    "\n",
    "            pred1 = pred.argmax(dim=1)\n",
    "\n",
    "\n",
    "            target3 = target3.squeeze()\n",
    "            loss_cass = criterion1(pred,target3)\n",
    "            # print('********************++++++++++++++++++++++************************++++++++++++++++++++++******************')\n",
    "            # print('pred1',pred1)\n",
    "            # print('label',target3)\n",
    "\n",
    "\n",
    "            true_test  = true_test+target3.numpy().tolist()\n",
    "            pred_test = pred_test+pred1.numpy().tolist()\n",
    "            pred_pro = pred_pro+pred.detach().numpy().tolist()\n",
    "            correct += torch.eq(pred1,target3).sum().float().item()\n",
    "            loss_comm_ = criterion(c1_, c2_)\n",
    "\n",
    "            loss_spe_ = criterion(s1_, s2_)\n",
    "\n",
    "            ll1_ = latent_loss(m1_, sig1_)\n",
    "            ll2_ = latent_loss(m2_, sig2_)\n",
    "\n",
    "            loss1_ = criterion(d1_, inputs3)\n",
    "            loss2_ = criterion(d2_, inputs4)\n",
    "            loss3_ = criterion(d3_, inputs3)\n",
    "            loss4_ = criterion(d4_, inputs4)\n",
    "            recon_ = loss1_ + loss2_ + loss3_ + loss4_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print('true',true_test)\n",
    "        print('pred', pred_test)\n",
    "\n",
    "        print('prob',pred_pro)\n",
    "        pred_pro_ = [i[1] for i in pred_pro]\n",
    "        print('prob_', pred_pro_)\n",
    "        tp,fp,tn,fn = perf_measure(true_test,pred_test)\n",
    "        print('AUC',calAUC(pred_pro_,true_test))\n",
    "        print('ACC', correct / total)\n",
    "        print('SEN',tp/(fn+tp))\n",
    "        print('SPE',tn/(fp+tn))\n",
    "\n",
    "        if epoch ==49:\n",
    "                print(Hiddle)\n",
    "                print(len(Hiddle))\n",
    "                numpy.savetxt('0.csv', Hiddle, delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        auc_list.append(calAUC(pred_pro_,true_test))\n",
    "        acc_list.append(correct / total)\n",
    "        sen_list.append(tp/(fn+tp))\n",
    "        speci_list.append(tn/(fp+tn))\n",
    "\n",
    "        if (epoch+1)%1 ==0:\n",
    "            l_list.append(sum_recon)\n",
    "            l_list_.append(recon_)\n",
    "            com_list.append(sum_com)\n",
    "            com_list_.append(loss_comm_)\n",
    "            spe_list.append(sum_spe)\n",
    "            spe_list_.append(loss_spe_)\n",
    "            com_spe_list.append(sum_com_spe)\n",
    "            classfication.append(sum_class)\n",
    "            classfication_.append(loss_cass)\n",
    "            # print(sum_recon)\n",
    "\n",
    "        # print('epoch',epoch,params[0],params[1],params[2],params[3])\n",
    "        lam.append(params[0].tolist()[0])\n",
    "        beita.append(params[1].tolist()[0])\n",
    "        classfication_value_1 = params[2].tolist()[0]\n",
    "        gama.append(classfication_value_1)\n",
    "        print('epoch',epoch)\n",
    "        print(lam[epoch])\n",
    "        print(beita[epoch])\n",
    "        print(gama[epoch])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(vae, 'MLDRL.pkl')\n",
    "\n",
    "    com_title = ['com_1','com_2','com_3','com_4','com_5','com_6','sp1_1','sp1_2','sp1_3','sp1_4','sp2_1','sp2_2','sp2_3','sp2_4']\n",
    "    com_csv = pd.DataFrame(columns=com_title, data=latent_space.data.numpy())\n",
    "    com_csv.to_csv('latent_final.csv')\n",
    "    recon_show =[]\n",
    "    com_show = []\n",
    "    spe_show = []\n",
    "    com_spe_show = []\n",
    "    class_show =[]\n",
    "    for i in range(len(l_list)):\n",
    "        if((i+1)%1==0):\n",
    "            recon_show.append((l_list[i]+l_list[i-1]+l_list[i-2]+l_list[i-3])/4)\n",
    "            com_show.append((com_list[i]+com_list[i-1]+com_list[i-2]+com_list[i-3])/4)\n",
    "            spe_show.append((spe_list[i]+spe_list[i-1]+spe_list[i-2]+spe_list[i-3])/4)\n",
    "            com_spe_show.append((com_spe_list[i]+com_spe_list[i-1]+com_spe_list[i-2]+com_spe_list[i-3])/4)\n",
    "            class_show.append((classfication[i]+classfication[i-1]+classfication[i-2]+classfication[i-3])/4)\n",
    "\n",
    "    x_range = range(0, 50)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.plot(x_range, auc_list, '-',label = 'auc')\n",
    "    plt.plot(x_range, acc_list, '-',label = 'acc')\n",
    "    plt.plot(x_range, speci_list, '-',label = 'spe')\n",
    "    plt.plot(x_range, sen_list, '-',label = 'sen')\n",
    "    plt.legend()\n",
    "    plt.title('recons')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_range = range(0,50)\n",
    "    plt.subplot(1,1,1)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.plot(x_range,l_list,'1-')\n",
    "    plt.plot(x_range, l_list_, '1-')\n",
    "\n",
    "    plt.title('recons')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "    x_range = range(0, 50)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.plot(x_range, com_list, '1-')\n",
    "    plt.plot(x_range, com_list_, '1-')\n",
    "\n",
    "    plt.title('com loss')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    x_range = range(0, 50)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.plot(x_range, spe_list,'1-')\n",
    "    plt.plot(x_range, spe_list_, '1-')\n",
    "\n",
    "    plt.title('spe loss')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    x_range = range(0, 50)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.plot(x_range, classfication, '1-')\n",
    "    plt.plot(x_range, classfication_, '1-')\n",
    "\n",
    "    plt.title('class loss')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "    recon_result = []\n",
    "    com_result =[]\n",
    "    spe_result =[]\n",
    "    class_result = []\n",
    "\n",
    "    recon_result_ = []\n",
    "    com_result_ = []\n",
    "    spe_result_ = []\n",
    "    class_result_ = []\n",
    "\n",
    "    lam_result = []\n",
    "    beita_result = []\n",
    "    gama_result = []\n",
    "\n",
    "\n",
    "    print(type(l_list))\n",
    "    for i in range(len(l_list)):\n",
    "        recon_result.append(l_list[i].tolist())\n",
    "        com_result.append(com_list[i].tolist())\n",
    "        spe_result.append(spe_list[i].tolist())\n",
    "        class_result.append(classfication[i].tolist())\n",
    "\n",
    "        recon_result_.append(l_list_[i].tolist())\n",
    "        com_result_.append(com_list_[i].tolist())\n",
    "        spe_result_.append(spe_list_[i].tolist())\n",
    "        class_result_.append(classfication_[i].tolist())\n",
    "\n",
    "        lam_result.append(lam[i])\n",
    "        beita_result.append(beita[i])\n",
    "        gama_result.append(gama[i])\n",
    "\n",
    "    print('l_list',recon_result)\n",
    "    print('com_list', com_result)\n",
    "    print('spe_list', spe_result)\n",
    "    print('class', class_result)\n",
    "\n",
    "    print(\"**********\")\n",
    "\n",
    "    print('l_list_', recon_result_)\n",
    "    print('com_list_', com_result_)\n",
    "    print('spe_list_', spe_result_)\n",
    "    print('class_', class_result_)\n",
    "\n",
    "    print(\"***********\")\n",
    "    print(lam_result)\n",
    "    print(beita_result)\n",
    "    print(gama_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
